{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Problem_3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryceXqTFDUFo"
      },
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "import statistics\n",
        "from sklearn.utils import shuffle"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEOIDk7aDdxC"
      },
      "source": [
        "import load_data as ld\n",
        "(train_data , train_labels , validation_data , validation_labels , test_data , test_labels) = ld.load()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5ySHH0MTxip"
      },
      "source": [
        "np.random.seed(1234)\n",
        "\n",
        "class FeedForwardNeuralNetwork():\n",
        "\n",
        "  def __init__(self, input_dim, output_dim, hidden_dim, hidden_layers):\n",
        "    self.input_dim = input_dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.output_dim = output_dim\n",
        "    self.hidden_layers = hidden_layers\n",
        "    self.L = hidden_layers\n",
        "    self.initialize()\n",
        "\n",
        "  def size(self):\n",
        "    l_sizes = []\n",
        "    l_sizes.append(self.input_dim)\n",
        "    for m in range(self.hidden_layers):\n",
        "      l_sizes.append(self.hidden_dim)\n",
        "    l_sizes.append(self.output_dim)\n",
        "    return l_sizes\n",
        "\n",
        "\n",
        "  def initialize(self):\n",
        "    size=self.size()\n",
        "    self.A = [np.zeros((size[i + 1], 1)) for i in range(len(size) - 1)]\n",
        "    self.H = [np.zeros((size[i + 1], 1)) for i in range(len(size) - 1)]\n",
        "    self.Weights=[np.random.randn(size[i + 1], size[i])*np.sqrt(2/(size[i]+size[i+1])) for i in range(len(size) - 1)]\n",
        "    self.Update_Weights=[np.zeros((size[i + 1], size[i])) for i in range(len(size) - 1)]\n",
        "    self.Biases= [np.zeros((size[i + 1], 1)) for i in range(len(size) - 1)]\n",
        "    self.Update_Biases= [np.zeros((size[i + 1], 1)) for i in range(len(size) - 1)]\n",
        "\n",
        "  def sigmoid(self, x):\n",
        "    return np.where(x >= 0, 1 / (1 + np.exp(-x)),np.exp(x) / (1 + np.exp(x)))\n",
        "\n",
        "  def diff_sigmoid(self,x):\n",
        "    fx = self.sigmoid(x)\n",
        "    return fx * (1 - fx)\n",
        "\n",
        "  def preactivation(self, w, h_prev, b):\n",
        "    return (np.dot(w,h_prev) + b)\n",
        "\n",
        "  def output_function(self, x): #stable_softmax\n",
        "    z=x-x.max()\n",
        "    num=np.exp(z)\n",
        "    denom=np.sum(num,axis=0)\n",
        "    return num/denom\n",
        "\n",
        "  def forwardPropagation(self, input_vector):\n",
        "    L = self.L   #total layers - input layer\n",
        "    self.A[0] = self.Biases[0] + np.dot(self.Weights[0], input_vector)\n",
        "\n",
        "    self.H[0] = self.sigmoid(self.A[0])\n",
        "\n",
        "    for k in range(1,L):\n",
        "      self.A[k] = np.array(self.preactivation(self.Weights[k], self.H[k-1], self.Biases[k]))\n",
        "      self.H[k] = self.sigmoid(self.A[k])\n",
        "\n",
        "    self.A[L] = np.array(self.preactivation(self.Weights[L], self.H[L-1], self.Biases[L]))\n",
        "    self.H[L] = self.output_function(self.A[L])\n",
        "\n",
        "    return self.H[L] #H[L] = y^\n",
        "\n",
        "\n",
        "  def backPropagation(self, train_data,truelabel,prediction):\n",
        "    L = self.L\n",
        "    e = np.zeros((10,1))\n",
        "    e[truelabel] = 1\n",
        "    grad_Weights = [0]*(L+1)\n",
        "    grad_Biases = [0]*(L+1)\n",
        "    # a = -(e - self.H[L])\n",
        "    a = -(e - prediction)\n",
        "    for k in range(L, -1, -1):\n",
        "      if k==0:\n",
        "        grad_Weights[k] = a.dot((train_data).T)\n",
        "      else:\n",
        "        grad_Weights[k] = a.dot((self.H[k-1]).T)\n",
        "\n",
        "      grad_Biases[k] = a\n",
        "      if k!=0:\n",
        "        second = self.diff_sigmoid(self.A[k-1])\n",
        "        first = (self.Weights[k].T).dot(a)\n",
        "        a =  np.multiply(first,second)\n",
        "\n",
        "    return (grad_Weights,grad_Biases)\n",
        "\n",
        "  def oneHot(self, num, size = 10):\n",
        "    vec = [0]*size\n",
        "    # print(\"num :\",num)\n",
        "    vec[num] = 1\n",
        "    vec = np.array(vec)\n",
        "    vec = vec.reshape(len(vec),1)\n",
        "    return vec\n",
        "\n",
        "\n",
        "  def calculate_error(self, test_data, test_labels):\n",
        "    predictions = []\n",
        "    count = 0\n",
        "    for example in test_data:\n",
        "      predictions.append(self.forwardPropagation(example.reshape(784,1)))\n",
        "    predicted_labels = list(map(np.argmax, predictions))\n",
        "    for p,l in zip(predicted_labels, test_labels):\n",
        "      if p == l:\n",
        "        count += 1\n",
        "    # print(\"accuracy% : \", (count/len(test_data))*100)\n",
        "    return (count/len(test_data))*100\n",
        "\n",
        "  #============================================================================================================================================================================================================================================\n",
        "  def trainingAlgo(self, train_data, train_labels, validation_data, validation_labels, opt = 'adam', gamma = 0.9, eta = 1e-4, batch_size = 1, max_epochs = 1,alpha = 0.1, eps = 1e-6, beta = 0.9):\n",
        "\n",
        "    limit = len(train_data)\n",
        "    # limit = 500\n",
        "    N = train_data.shape[0]\n",
        "    L = self.L\n",
        "    # prev_w, prev_b = [0]*(L+1), [0]*(L+1)\n",
        "    # v_w, v_b = [0]*(L+1), [0]*(L+1)\n",
        "\n",
        "    if opt=='adam' or 'nadam':\n",
        "      m_w,m_b = [0]*(L+1),[0]*(L+1)\n",
        "      step = 1\n",
        "\n",
        "    for i in range(max_epochs):\n",
        "      g_w, g_b = [0]*(L+1), [0]*(L+1)\n",
        "      count = 0\n",
        "      X_train,y_train = shuffle(train_data,train_labels,random_state=0)\n",
        "      # for x, y in zip(X_train, y_train):\n",
        "      for x, y in zip(X_train[:limit], y_train[:limit]):\n",
        "        print(\"=====================================================================================================\")\n",
        "        print(\"training for example number : \", count)\n",
        "        print(\"\\n\")\n",
        "        predictions=self.forwardPropagation(x.reshape(784,1))\n",
        "        (grad_Weights, grad_Biases) = self.backPropagation(x.reshape(784,1),y,predictions)\n",
        "\n",
        "        for j in range(L+1):\n",
        "          g_w[j] = g_w[j] + grad_Weights[j]\n",
        "          g_b[j] = g_b[j] + grad_Biases[j]\n",
        "        count = count + 1\n",
        "\n",
        "        if count % batch_size == 0 or count == N :\n",
        "\n",
        "          if opt=='momentum':\n",
        "            for j in range(0,L+1):\n",
        "              self.Update_Weights[j] = gamma * self.Update_Weights[j] + (eta * grad_Weights[j])\n",
        "              self.Weights[j] =self.Weights[j]-self.Update_Weights[j]\n",
        "\n",
        "              self.Update_Biases[j] = gamma * self.Update_Biases[j] + (eta * grad_Biases[j])\n",
        "              self.Biases[j]  = self.Biases[j] - self.Update_Biases[j]\n",
        "\n",
        "          elif opt=='sgd':\n",
        "            for j in range(L + 1):\n",
        "              self.Weights[j]   = self.Weights[j]-(eta*grad_Weights[j])\n",
        "\n",
        "              self.Biases[j]  = self.Biases[j]-(eta * grad_Biases[j])\n",
        "\n",
        "          elif opt=='nestrov':\n",
        "            for j in range(L + 1):\n",
        "              self.Update_Weights[j] = (gamma * self.Update_Weights[j]) + (eta * g_w[j])\n",
        "              self.Weights[j]  = self.Weights[j] - ( gamma * prev_w[j] + eta * g_w[j] )\n",
        "\n",
        "              self.Update_Biases[j] = (gamma *  self.Update_Biases[j]) + (eta * g_b[j])\n",
        "              self.Biases[j]  = self.Biases[j]- ( gamma *  self.Update_Biases[j] + eta * g_b[j] )\n",
        "\n",
        "          elif opt=='rmsprop':\n",
        "            for j in range(L + 1):\n",
        "              self.Update_Weights[j] = beta * self.Update_Weights[j] + (1 - beta) * g_w[j]**2\n",
        "              self.Weights[j] = ( 1 - eta*alpha ) * self.Update_Weights[j] - eta /( np.sqrt(self.Update_Weights[j])+ eps) * grad_Weights[j]\n",
        "\n",
        "              self.Update_Biases[j]= beta *  self.Update_Biases[j] + (1 - beta) * g_b[j]**2\n",
        "              self.Biases[j]  = self.Biases[j] -(eta / (np.sqrt( self.Update_Biases[j]) +eps)) * grad_Biases[j]\n",
        "\n",
        "          elif opt=='adam':\n",
        "            # pass\n",
        "            beta1 = 0.9\n",
        "            beta2 = 0.999\n",
        "            for j in range(L + 1):\n",
        "\n",
        "              self.Update_Weights[j] = beta2 * self.Update_Weights[j] + (1 - beta2) * g_w[j]**2\n",
        "              m_w[j]=beta1*m_w[j] +(1-beta1) * g_w[j]\n",
        "              m_w_hat=m_w[j]/(1-math.pow(beta1,step))\n",
        "              v_w_hat=self.Update_Weights[j]/(1-math.pow(beta2,step))\n",
        "              self.Weights[j]=(1-eta*alpha/N)*self.Weights[j] -(eta/(np.sqrt(v_w_hat)+eps))*m_w_hat\n",
        "\n",
        "              self.Update_Biases[j] = beta2 * self.Update_Biases[j] + (1 - beta2) * g_b[j]**2\n",
        "              m_b[j]=beta1*m_b[j] +(1-beta1) * g_b[j]\n",
        "              m_b_hat=m_b[j]/(1-math.pow(beta1,step))\n",
        "              v_b_hat=self.Update_Biases[j]/(1-math.pow(beta2,step))\n",
        "              self.Biases[j] = self.Biases[j] -(eta / (np.sqrt(v_b_hat)+eps))* m_b_hat\n",
        "\n",
        "          elif opt=='nadam':\n",
        "            # pass\n",
        "            beta1 = 0.9\n",
        "            beta2 = 0.999\n",
        "            for j in range(L + 1):\n",
        "              self.Update_Weights[j] = beta2 * self.Update_Weights[j] + (1 - beta2) * g_w[j] ** 2\n",
        "              m_w[j] = beta1 * m_w[j] + (1 - beta1) * g_w[j]\n",
        "              m_w_hat = m_w[j] / (1 - math.pow(beta1, step))\n",
        "              m_w_hat = beta1 * m_w_hat + ((1 - beta1) * g_w[j]) / (1 - math.pow(beta1, step))\n",
        "              v_w_hat=self.Update_Weights[j]/(1-math.pow(beta2,step))\n",
        "              self.Weights[j] = (1 - eta * alpha) * self.Weights[j] -(eta /( np.sqrt(v_w_hat) + eps)) * m_w_hat\n",
        "\n",
        "              self.Update_Biases[j] = beta2 * self.Update_Biases[j] + (1 - beta2) * g_b[j] ** 2\n",
        "              m_b[j] = beta1 * m_b[j] + (1 - beta1) * g_b[j]\n",
        "              m_b_hat = beta1 * (m_b[j] / (1 - math.pow(beta1, step))) + ((1 - beta1) * g_b[j]) / (1 - math.pow(beta1, step))\n",
        "              v_b_hat=self.Update_Biases[j]/(1-math.pow(beta2,step))\n",
        "              self.Biases[j] = self.Biases[j]-(eta/(np.sqrt(v_b_hat)+eps))*m_b_hat\n",
        "\n",
        "          else:\n",
        "            pass\n",
        "\n",
        "          g_w=[0]*(L+1)\n",
        "          g_b=[0]*(L+1)\n",
        "          step = step + 1\n",
        "      train_acc = self.calculate_error(train_data, train_labels)\n",
        "      val_acc = self.calculate_error(validation_data, validation_labels)\n",
        "      print(\"train_acc : \",train_acc, \", val_acc : \", val_acc, \" epoch : \", i)\n",
        "\n",
        "    return\n",
        "\n",
        "  #============================================================================================================================================================================================================================================\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKl_eIeX2KIY"
      },
      "source": [
        "input_dim = train_data[0].reshape(784,1).shape[0]\n",
        "output_dim, hidden_dim, hidden_layers = 10, input_dim, 8"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N91YQz252KeC"
      },
      "source": [
        "eta = 1e-4\n",
        "gamma = 0.9\n",
        "optimizer = 'sgd'\n",
        "batch_size = 1\n",
        "max_epochs = 1\n",
        "number_of_neurons_per_hidden_layer = 128\n",
        "number_of_hidden_layers = 1\n",
        "\n",
        "#=================================================================================================================================\n",
        "# BUILDING AND TRAINING THE NEURAL NETWORK\n",
        "#---------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "ffnn = FeedForwardNeuralNetwork(784, 10, number_of_neurons_per_hidden_layer, number_of_hidden_layers)\n",
        "ffnn.trainingAlgo(train_data, train_labels, validation_data, validation_labels, optimizer, gamma, eta, batch_size, max_epochs)\n",
        "print(\"testing accuracy : \",ffnn.calculate_error(test_data, test_labels))\n",
        "\n",
        "#================================================================================================================================="
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}