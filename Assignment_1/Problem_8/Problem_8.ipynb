{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Problem_8.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5B4OrLTCTinA"
      },
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "from matplotlib import pyplot as plt \n",
        "from sklearn.utils import shuffle\n",
        "import numpy as np\n",
        "import statistics\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChZv24keT3ds"
      },
      "source": [
        "import load_data as ld\n",
        "(train_data , train_labels , validation_data , validation_labels , test_data , test_labels) = ld.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BGS8mhJT3bc"
      },
      "source": [
        "class FeedForwardNeuralNetwork():\n",
        "\n",
        "###################################################### Constructor ################################################################\n",
        "\n",
        "  def __init__(self, input_dim, output_dim, hidden_dim, hidden_layers, activation = \"sigmoid\", weight_intialisation = \"random\"):\n",
        "    np.random.seed(1234)\n",
        "    self.input_dim = input_dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.output_dim = output_dim\n",
        "    self.hidden_layers = hidden_layers\n",
        "    self.L = hidden_layers\n",
        "    self.activation = activation\n",
        "    self.weight_initialisation = weight_initialisation\n",
        "    self.initialize()\n",
        "\n",
        "\n",
        "##################################### Function for storing dimensions of all the layers ############################################\n",
        "\n",
        "  def size(self):\n",
        "    l_sizes = []\n",
        "    l_sizes.append(self.input_dim)\n",
        "    for m in range(self.hidden_layers):\n",
        "      l_sizes.append(self.hidden_dim)\n",
        "    l_sizes.append(self.output_dim)\n",
        "    return l_sizes\n",
        "\n",
        "\n",
        "######################################## Function for initialization ###########################################3\n",
        "\n",
        "  def initialize(self):\n",
        "    size = self.size()    \n",
        "\n",
        "#-------------------------- random weight-initialization ---------------------------------\n",
        "\n",
        "    if self.weight_initialisation == \"random\":\n",
        "      self.Weights=[np.random.randn(size[i + 1], size[i]) for i in range(len(size) - 1)]   \n",
        "\n",
        "#--------------------------- xavier weight-initialization --------------------------------\n",
        "\n",
        "    if self.weight_initialisation == \"xavier\":\n",
        "      self.Weights=[np.random.randn(size[i + 1], size[i])*np.sqrt(2/(size[i]+size[i+1])) for i in range(len(size) - 1)]\n",
        "\n",
        "#--------------Initialize Biases, Activation, Preactivation, update weights and update biases ndarray  ----------------------\n",
        "\n",
        "    self.Update_Weights=[np.zeros((size[i + 1], size[i])) for i in range(len(size) - 1)]\n",
        "    self.Biases= [np.zeros((size[i + 1], 1)) for i in range(len(size) - 1)]\n",
        "    self.Update_Biases= [np.zeros((size[i + 1], 1)) for i in range(len(size) - 1)]\n",
        "    self.A = [np.zeros((size[i + 1], 1)) for i in range(len(size) - 1)]\n",
        "    self.H = [np.zeros((size[i + 1], 1)) for i in range(len(size) - 1)]\n",
        "  \n",
        " ######################################### Activation Function ####################################################################\n",
        "  \n",
        "  def activation_function(self, x, activation = \"sigmoid\"):\n",
        "    if activation == \"sigmoid\":\n",
        "      return np.where(x >= 0, 1 / (1 + np.exp(-x)),np.exp(x) / (1 + np.exp(x)))\n",
        "\n",
        "    if activation == \"tanh\":\n",
        "      return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
        "\n",
        "    if activation == \"relu\":\n",
        "      # return np.where(x>=0,x,0)\n",
        "      return np.where(x>=0,0.12*x,0)\n",
        "\n",
        "\n",
        "    if activation == \"lrelu\":\n",
        "      return np.where(x>0,x,0.2*x)\n",
        "      # temp = np.where(x>0,x,0)\n",
        "      # print(\"temp in relu : \",temp)\n",
        "      # return np.where(x>0,x,0)\n",
        "\n",
        "\n",
        " ######################################### Function for finding derivative of activation  ######################################\n",
        "\n",
        "  def diff_activation_function(self, x, activation = \"sigmoid\"):\n",
        "    if activation == \"sigmoid\":\n",
        "      fx = np.where(x >= 0, 1 / (1 + np.exp(-x)),np.exp(x) / (1 + np.exp(x)))\n",
        "      return fx * (1 - fx)\n",
        "\n",
        "    if activation == \"tanh\":\n",
        "      fx = (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
        "      return (1 - np.power((fx),2))\n",
        "\n",
        "    if activation == \"relu\":\n",
        "      return np.where(x>=0,0.12,0) \n",
        "\n",
        "    if activation == \"lrelu\":\n",
        "      return np.where(x>=0,1,0.2)\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "######################################### Prectivation Function ###################################################\n",
        "\n",
        "\n",
        "  def preactivation(self, w, h_prev, b):\n",
        "    return (np.dot(w,h_prev) + b)\n",
        "\n",
        "##################################### Output Function using stable Softmax #####################################################\n",
        "\n",
        "\n",
        "  def output_function(self, x): \n",
        "    z=x-x.max()\n",
        "    num=np.exp(z)\n",
        "    denom=np.sum(num,axis=0)\n",
        "    return num/denom\n",
        "\n",
        "######################################### converting label to corresponding one hot vector  ###################################################\n",
        "\n",
        "  def oneHot(self, num, size = 10):\n",
        "    vec = [0]*size\n",
        "    # print(\"num :\",num)\n",
        "    vec[num] = 1\n",
        "    vec = np.array(vec)\n",
        "    vec = vec.reshape(len(vec),1)\n",
        "    return vec\n",
        "\n",
        "\n",
        "######################################### Calculate accuracy and losses of our model  ###################################################\n",
        "\n",
        "  def calculate_accuracy(self, test_data, test_labels, limit):\n",
        "    predictions = []\n",
        "    count = 0\n",
        "    for example in test_data[:limit]:\n",
        "      predictions.append(self.forwardPropagation(example.reshape(784,1)))\n",
        "    predicted_labels = list(map(np.argmax, predictions))\n",
        "    for p,l in zip(predicted_labels, test_labels[:limit]):\n",
        "      if p == l:\n",
        "        count += 1\n",
        "    # print(\"accuracy% : \", (count/len(test_data))*100)\n",
        "    return (count/limit)*100\n",
        "\n",
        "#--------------------------- cross entropy loss --------------------------------\n",
        "\n",
        "  def ce_loss(self, data, labels, limit):\n",
        "    predictions = []\n",
        "    count = 0\n",
        "    for example in data[:limit]:\n",
        "      predictions.append(self.forwardPropagation(example.reshape(784,1)))\n",
        "    #-log(predictions[labels[i]])\n",
        "    total_ce_loss = 0\n",
        "    for i in range(limit):\n",
        "      total_ce_loss += -math.log(predictions[i][labels[i]])\n",
        "    average_ce_loss = total_ce_loss / limit\n",
        "    return average_ce_loss\n",
        "\n",
        "#--------------------------- squared_error_loss --------------------------------\n",
        "\n",
        "  def squared_error_loss(self, data, labels, limit):\n",
        "    predictions = []\n",
        "    count = 0\n",
        "    for example in data[:limit]:\n",
        "      predictions.append(self.forwardPropagation(example.reshape(784,1)))\n",
        "    #\n",
        "    total_sqe_loss = 0\n",
        "    for i in range(limit):\n",
        "      total_sqe_loss += sum(np.power(self.oneHot(labels[i])-predictions[i],2))\n",
        "    average_sqe_loss = total_sqe_loss[0] / limit\n",
        "    return average_sqe_loss\n",
        "\n",
        "\n",
        "######################################### function for Forward propogation  ###################################################\n",
        "\n",
        "\n",
        "  def forwardPropagation(self, input_vector):\n",
        "    activation = self.activation\n",
        "    L = self.L   #total layers - input layer\n",
        "    self.A[0] = self.Biases[0] + np.dot(self.Weights[0], input_vector)\n",
        "    self.H[0] = self.activation_function(self.A[0], activation)\n",
        "\n",
        "    for k in range(1,L):\n",
        "      self.A[k] = np.array(self.preactivation(self.Weights[k], self.H[k-1], self.Biases[k]))\n",
        "      self.H[k] = self.activation_function(self.A[k], activation)\n",
        "\n",
        "    self.A[L] = np.array(self.preactivation(self.Weights[L], self.H[L-1], self.Biases[L]))\n",
        "    self.H[L] = self.output_function(self.A[L])\n",
        "\n",
        "    return self.H[L] #H[L] = y^\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "######################################### function for back propogation  ###################################################\n",
        "\n",
        "\n",
        "  def backPropagation(self, train_data,truelabel,prediction, loss_type):\n",
        "    \n",
        "    L = self.L\n",
        "    grad_Weights = [0]*(L+1)\n",
        "    grad_Biases = [0]*(L+1)\n",
        "    activation = self.activation\n",
        "    \n",
        "    a = None\n",
        "    if loss_type == \"sqe\":\n",
        "      a = np.power(self.oneHot(truelabel)-prediction,2)\n",
        "\n",
        "    if loss_type == \"ce\":     \n",
        "      e = np.zeros((10,1))\n",
        "      e[truelabel] = 1      \n",
        "      # a = -(e - self.H[L])\n",
        "      a = -(e - prediction)\n",
        "\n",
        "    for k in range(L, -1, -1):\n",
        "      if k==0:\n",
        "        grad_Weights[k] = a.dot((train_data).T)\n",
        "      else:\n",
        "        grad_Weights[k] = a.dot((self.H[k-1]).T)\n",
        "\n",
        "      grad_Biases[k] = a\n",
        "      if k!=0:\n",
        "        second = self.diff_activation_function(self.A[k-1], activation)\n",
        "        first = (self.Weights[k].T).dot(a)\n",
        "        a =  np.multiply(first,second)\n",
        "    \n",
        "\n",
        "    return (grad_Weights,grad_Biases)\n",
        "\n",
        "######################################### Training of our model  ###################################################\n",
        "\n",
        "  def trainingAlgo(self, opt = 'adam', gamma = 0.9, eta = 1e-4, batch_size = 1, max_epochs = 1,alpha = 0.1, eps = 1e-6, beta = 0.9, limit = 500, vlimit= 500, tlimit = 500, loss_type = \"ce\"):\n",
        "\n",
        "    (train_data , train_labels , validation_data , validation_labels , test_data , test_labels) = ld.load()\n",
        "\n",
        "    N = train_data.shape[0]\n",
        "    L = self.L\n",
        "    loss = []    \n",
        "\n",
        "    if opt=='adam' or 'nadam':\n",
        "      m_w,m_b = [0]*(L+1),[0]*(L+1)\n",
        "      step = 1\n",
        "\n",
        "    for i in range(max_epochs):\n",
        "      print(\"\\nEpoch : \", i+1)\n",
        "      g_w, g_b = [0]*(L+1), [0]*(L+1)\n",
        "      count = 0\n",
        "      X_train,y_train = shuffle(train_data,train_labels,random_state=0)\n",
        "      # for x, y in zip(X_train, y_train):\n",
        "      for x, y in zip(X_train[:limit], y_train[:limit]):\n",
        "        # print(\"\\ntraining for example number : \", count+1)\n",
        "\n",
        "        predictions=self.forwardPropagation(x.reshape(784,1))\n",
        "        (grad_Weights, grad_Biases) = self.backPropagation(x.reshape(784,1),y,predictions, loss_type)\n",
        "\n",
        "        for j in range(L+1):\n",
        "          g_w[j] = g_w[j] + grad_Weights[j]\n",
        "          g_b[j] = g_b[j] + grad_Biases[j]\n",
        "        count = count + 1\n",
        "\n",
        "        if count % batch_size == 0 or count == N :\n",
        "\n",
        "#-----------------------------update weights and biases for momentum based stochastic gradient descent optimizer-----------------------------------------\n",
        "          if opt=='momentum':\n",
        "            for j in range(0,L+1):\n",
        "              self.Update_Weights[j] = gamma * self.Update_Weights[j] + (eta * grad_Weights[j])\n",
        "              self.Weights[j] =self.Weights[j]-self.Update_Weights[j]\n",
        "\n",
        "              self.Update_Biases[j] = gamma * self.Update_Biases[j] + (eta * grad_Biases[j])\n",
        "              self.Biases[j]  = self.Biases[j] - self.Update_Biases[j]\n",
        "\n",
        "\n",
        "\n",
        "#-----------------------------update weights and biases for Stochastic gradient descent optimizer-----------------------------------------\n",
        "          elif opt=='sgd':\n",
        "            for j in range(L + 1):\n",
        "              self.Weights[j]   = self.Weights[j]-(eta*grad_Weights[j])\n",
        "              self.Biases[j]  = self.Biases[j]-(eta * grad_Biases[j])\n",
        "\n",
        "\n",
        "\n",
        "#-----------------------------update weights and biases for nestrov optimizer-----------------------------------------\n",
        "          elif opt=='nestrov':\n",
        "            for j in range(L + 1):\n",
        "              self.Update_Weights[j] = (gamma * self.Update_Weights[j]) + (eta * g_w[j])\n",
        "              self.Weights[j]  = self.Weights[j] - ( gamma * prev_w[j] + eta * g_w[j] )\n",
        "\n",
        "              self.Update_Biases[j] = (gamma *  self.Update_Biases[j]) + (eta * g_b[j])\n",
        "              self.Biases[j]  = self.Biases[j]- ( gamma *  self.Update_Biases[j] + eta * g_b[j] )\n",
        "\n",
        "\n",
        "\n",
        "#-----------------------------update weights and biases for RMSProp optimizer-----------------------------------------\n",
        "          elif opt=='rmsprop':\n",
        "            for j in range(L + 1):\n",
        "              self.Update_Weights[j] = beta * self.Update_Weights[j] + (1 - beta) * g_w[j]**2\n",
        "              self.Weights[j] = ( 1 - eta*alpha ) * self.Update_Weights[j] - eta /( np.sqrt(self.Update_Weights[j])+ eps) * grad_Weights[j]\n",
        "\n",
        "              self.Update_Biases[j]= beta *  self.Update_Biases[j] + (1 - beta) * g_b[j]**2\n",
        "              self.Biases[j]  = self.Biases[j] -(eta / (np.sqrt( self.Update_Biases[j]) +eps)) * grad_Biases[j]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#-----------------------------update weights and biases for Adam optimizer-----------------------------------------\n",
        "          elif opt=='adam':\n",
        "            # pass\n",
        "            beta1 = 0.9\n",
        "            beta2 = 0.999\n",
        "            for j in range(L + 1):\n",
        "\n",
        "              self.Update_Weights[j] = beta2 * self.Update_Weights[j] + (1 - beta2) * g_w[j]**2\n",
        "              m_w[j]=beta1*m_w[j] +(1-beta1) * g_w[j]\n",
        "              m_w_hat=m_w[j]/(1-math.pow(beta1,step))\n",
        "              v_w_hat=self.Update_Weights[j]/(1-math.pow(beta2,step))\n",
        "              self.Weights[j]=(1-eta*alpha/N)*self.Weights[j] -(eta/(np.sqrt(v_w_hat)+eps))*m_w_hat\n",
        "\n",
        "              self.Update_Biases[j] = beta2 * self.Update_Biases[j] + (1 - beta2) * g_b[j]**2\n",
        "              m_b[j]=beta1*m_b[j] +(1-beta1) * g_b[j]\n",
        "              m_b_hat=m_b[j]/(1-math.pow(beta1,step))\n",
        "              v_b_hat=self.Update_Biases[j]/(1-math.pow(beta2,step))\n",
        "              self.Biases[j] = self.Biases[j] -(eta / (np.sqrt(v_b_hat)+eps))* m_b_hat\n",
        "\n",
        "\n",
        "\n",
        "#-----------------------------updating weights and biases for Nadam optimizer-----------------------------------------\n",
        "          elif opt=='nadam':\n",
        "            # pass\n",
        "            beta1 = 0.9\n",
        "            beta2 = 0.999\n",
        "            for j in range(L + 1):\n",
        "              self.Update_Weights[j] = beta2 * self.Update_Weights[j] + (1 - beta2) * g_w[j] ** 2\n",
        "              m_w[j] = beta1 * m_w[j] + (1 - beta1) * g_w[j]\n",
        "              m_w_hat = m_w[j] / (1 - math.pow(beta1, step))\n",
        "              m_w_hat = beta1 * m_w_hat + ((1 - beta1) * g_w[j]) / (1 - math.pow(beta1, step))\n",
        "              v_w_hat=self.Update_Weights[j]/(1-math.pow(beta2,step))\n",
        "              self.Weights[j] = (1 - eta * alpha) * self.Weights[j] -(eta /( np.sqrt(v_w_hat) + eps)) * m_w_hat\n",
        "\n",
        "              self.Update_Biases[j] = beta2 * self.Update_Biases[j] + (1 - beta2) * g_b[j] ** 2\n",
        "              m_b[j] = beta1 * m_b[j] + (1 - beta1) * g_b[j]\n",
        "              m_b_hat = beta1 * (m_b[j] / (1 - math.pow(beta1, step))) + ((1 - beta1) * g_b[j]) / (1 - math.pow(beta1, step))\n",
        "              v_b_hat=self.Update_Biases[j]/(1-math.pow(beta2,step))\n",
        "              self.Biases[j] = self.Biases[j]-(eta/(np.sqrt(v_b_hat)+eps))*m_b_hat\n",
        "\n",
        "          else:\n",
        "            pass\n",
        "\n",
        "          g_w=[0]*(L+1)\n",
        "          g_b=[0]*(L+1)\n",
        "          step = step + 1\n",
        "      train_acc = self.calculate_accuracy(train_data, train_labels, limit)\n",
        "      val_acc = self.calculate_accuracy(validation_data, validation_labels, vlimit)\n",
        "      test_acc = self.calculate_accuracy(test_data, test_labels, tlimit)\n",
        "\n",
        "      # 8Q ans)\n",
        "      if loss_type == \"ce\":\n",
        "        train_ce_loss = self.ce_loss(train_data, train_labels, limit)\n",
        "        valid_ce_loss = self.ce_loss(validation_data, validation_labels, vlimit)\n",
        "        test_ce_loss = self.ce_loss(test_data, test_labels, tlimit)\n",
        "        loss.append(test_ce_loss)\n",
        "      if loss_type == \"sqe\":\n",
        "        test_sqe_loss = self.squared_error_loss(test_data, test_labels, tlimit)\n",
        "        loss.append(test_sqe_loss)\n",
        "     \n",
        "\n",
        "    return loss\n",
        "\n",
        "  #============================================================================================================================================================================================================================================\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNBppe7lUoxl"
      },
      "source": [
        "number_of_hidden_layers = 1\n",
        "number_of_neurons_in_hidden_layer = 128\n",
        "activation_function = \"sigmoid\"\n",
        "weight_initialisation = \"xavier\"\n",
        "optimiser = \"adam\"\n",
        "gamma = 0.9\n",
        "eta = 1e-4\n",
        "batch_size = 1\n",
        "max_epochs = 1\n",
        "alpha = 0.005\n",
        "train_limit = len(train_data)\n",
        "test_limit = len(test_data)\n",
        "validation_limit = len(validation_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RK-dScChUoub"
      },
      "source": [
        "#===========================================================================Run without WandB ====================================================================================================\n",
        "\n",
        "\n",
        "number_of_hidden_layers = 4\n",
        "number_of_neurons_in_hidden_layer = 128\n",
        "activation_function = \"tanh\"\n",
        "weight_initialisation = \"xavier\"\n",
        "optimiser = \"adam\"\n",
        "gamma = 0.9\n",
        "eta = 1e-3\n",
        "batch_size = 64\n",
        "max_epochs = 10\n",
        "alpha = 0.5\n",
        "loss_type = \"ce\" # sqe\n",
        "train_limit = len(train_data)\n",
        "test_limit = len(test_data)\n",
        "validation_limit = len(validation_data)\n",
        "# train_limit = 500\n",
        "# test_limit = 500\n",
        "# validation_limit = 500\n",
        "\n",
        "ffnn1 = FeedForwardNeuralNetwork(784, 10, number_of_neurons_in_hidden_layer, number_of_hidden_layers, activation_function, weight_initialisation)\n",
        "loss1 = ffnn1.trainingAlgo(optimiser, gamma, eta, batch_size, max_epochs, alpha, limit = train_limit, vlimit = validation_limit, tlimit = test_limit, loss_type = \"ce\")\n",
        "print(\"==================================================================================================================================================================================================\")\n",
        "ffnn2 = FeedForwardNeuralNetwork(784, 10, number_of_neurons_in_hidden_layer, number_of_hidden_layers, activation_function, weight_initialisation)\n",
        "loss2 = ffnn2.trainingAlgo(optimiser, gamma, eta, batch_size, max_epochs, alpha, limit = train_limit, vlimit = validation_limit, tlimit = test_limit, loss_type = \"sqe\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3C3YAfHm2FUW",
        "outputId": "18044305-9624-4926-c4c7-e1f614324cca"
      },
      "source": [
        "print(\"cross_entropy loss: \",loss1)\n",
        "print(\"==========================================================================================\")\n",
        "print(\"square error loss : \",loss2)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cross_entropy loss:  [0.4363928285676967, 0.4024874005725422, 0.3678191201017786, 0.36184962400836473, 0.3693317687361954, 0.3734739970089634, 0.3919448044911578, 0.3777499754588874, 0.38155280911819184, 0.377931755364064]\n",
            "==========================================================================================\n",
            "square error loss :  [1.2727956927413053, 1.280001340214756, 1.2865072438889784, 1.290270044017152, 1.2961659445478908, 1.2979260313382794, 1.2977162217409872, 1.3002190898429828, 1.3047412477591056, 1.3035950728161627]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ua3IVn6TJmoD"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xoO1STBTJ-4n",
        "outputId": "fd78c7ff-cd1d-4cfe-c7d7-984829b776c9"
      },
      "source": [
        "X = list(range(1,len(loss1)+1))\n",
        "# a = plt.scatter(X,loss1)\n",
        "# plt.plot(a, X, loss1)\n",
        "# plt.show()\n",
        "plt.plot(X, loss1, '*', label='cross_entropy_loss')\n",
        "plt.plot(X, loss2, 'o', label='square_error_loss')\n",
        "plt.legend(loc='best')\n",
        "plt.title('Comparison of cross entropy loss and squared error loss')\n",
        "plt.ylabel('error')\n",
        "plt.xlabel('epochs')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}