{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "a3_Problem_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OlH8H6Dgseh"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import cv2\n",
        "import pathlib\n",
        "import glob\n",
        "import shutil\n",
        "import os\n",
        "import random"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STMGDOBCguDe",
        "outputId": "084d18a4-1585-4654-d7c3-2ab8465bdb0f"
      },
      "source": [
        "#--------------------------------caution: terminal commands ---------------------------------------------\n",
        "%cd\n",
        "%cd .keras/datasets/\n",
        "!rm -r *"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root\n",
            "/root/.keras/datasets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kExxbfhyguAW",
        "outputId": "873c9f55-e151-4847-d0d7-bdf9e97e0f86"
      },
      "source": [
        "########################################### download data from given url ###############################################\n",
        "\n",
        "dataset_url = \"https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\"\n",
        "data_dir = tf.keras.utils.get_file('dakshina_dataset_v1.0', origin=dataset_url, untar=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
            "2008342528/2008340480 [==============================] - 19s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsOhKufxgt7-",
        "outputId": "f19213b4-0edb-4a12-8b16-84af5b4593c7"
      },
      "source": [
        "#----------------------------------terminal command -----------------------------------------------\n",
        "%cd /root/.keras/datasets/dakshina_dataset_v1.0/hi/lexicons/\n",
        "!ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/.keras/datasets/dakshina_dataset_v1.0/hi/lexicons\n",
            "hi.translit.sampled.dev.tsv   hi.translit.sampled.train.tsv\n",
            "hi.translit.sampled.test.tsv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxSiYbw8hpcH"
      },
      "source": [
        "train_data_path = \"hi.translit.sampled.train.tsv\"\n",
        "test_data_path = \"hi.translit.sampled.test.tsv\"\n",
        "validation_data_path = \"hi.translit.sampled.dev.tsv\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ihHzaQ5KXj_"
      },
      "source": [
        "**UTILITY FUNCTIONS FOR PREPOCESSING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iqFHMS9Msbd"
      },
      "source": [
        "################################# function for vectorizing the data ##########################################\n",
        "\n",
        "def vectorize_data(train_data_path):\n",
        "  input_texts = []\n",
        "  target_texts = []\n",
        "  input_characters = set()\n",
        "  target_characters = set()\n",
        "  with open(train_data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.read().split(\"\\n\")\n",
        "  for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "    target_text, input_text, _ = line.split(\"\\t\")\n",
        "    #---------------We use \"tab\" as the \"start sequence\" character---------------------\n",
        "    #----------------for the targets, and \"\\n\" as \"end sequence\" character-----------------.\n",
        "    target_text = \"\\t\" + target_text + \"\\n\"\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "    for char in input_text:\n",
        "        if char not in input_characters:\n",
        "            input_characters.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in target_characters:\n",
        "            target_characters.add(char)\n",
        "\n",
        "  #--------------------------------artificially added-----------------------------\n",
        "  input_characters.add(\" \")\n",
        "  target_characters.add(\" \")\n",
        "\n",
        "  input_characters = sorted(list(input_characters))\n",
        "  target_characters = sorted(list(target_characters))\n",
        "\n",
        "  num_encoder_tokens = len(input_characters)\n",
        "  num_decoder_tokens = len(target_characters)\n",
        "  max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "  max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "  print(\"Number of samples:\", len(input_texts))\n",
        "  print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "  print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "  print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "  print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
        "\n",
        "  input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
        "  target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
        "\n",
        "  input_details = [input_characters, input_texts, input_token_index, num_encoder_tokens, max_encoder_seq_length]\n",
        "  target_details = [target_characters, target_texts, target_token_index, num_decoder_tokens, max_decoder_seq_length]\n",
        "\n",
        "  return (input_details, target_details)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MK_dT-20VMk1"
      },
      "source": [
        "################### function for converting the data into apropriate ONE-Hot vector ######################\n",
        "\n",
        "def onehot(input_details, target_details):\n",
        "\n",
        "    #---------------------------unzipping information-----------------------------------\n",
        "    input_characters = input_details[0]\n",
        "    input_texts = input_details[1]\n",
        "    input_token_index = input_details[2]\n",
        "    num_encoder_tokens = input_details[3]\n",
        "    max_encoder_seq_length = input_details[4]\n",
        "\n",
        "    target_characters = target_details[0]\n",
        "    target_texts = target_details[1]\n",
        "    target_token_index = target_details[2]\n",
        "    num_decoder_tokens = target_details[3]\n",
        "    max_decoder_seq_length = target_details[4]\n",
        "\n",
        "    #---------------------------- creating 3-Dim  matrics with all entries = 0 ----------------------------------- \n",
        "    encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\")\n",
        "    decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n",
        "    decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n",
        "\n",
        "    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "        for t, char in enumerate(input_text):\n",
        "            encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
        "\n",
        "        encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
        "        for t, char in enumerate(target_text):\n",
        "# --------------decoder_target_data is ahead of decoder_input_data by one timestep ----------------------------\n",
        "            decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
        "            \n",
        "            if t > 0:\n",
        "# ----------------decoder_target_data will be ahead by one timestep----------------------------------\n",
        "                decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "\n",
        "        decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n",
        "        decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0\n",
        "    \n",
        "    return (encoder_input_data, decoder_input_data, decoder_target_data)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Es6uD_3JJT7p"
      },
      "source": [
        "################### function for creating data into appropriate form required for embedding ###################\n",
        "\n",
        "def get_input_for_embedding(input_details, embed_size, train_details = None):\n",
        "\n",
        "  #---------------------------unzipping information-----------------------------------\n",
        "  input_characters = input_details[0]\n",
        "  input_texts = input_details[1]\n",
        "  input_token_index = input_details[2]\n",
        "  num_encoder_tokens = input_details[3]\n",
        "  max_encoder_seq_length = input_details[4]\n",
        "\n",
        "  if train_details != None:\n",
        "    input_token_index = train_details[2]\n",
        "  \n",
        "  input_array = np.zeros((len(input_texts), max_encoder_seq_length), dtype=\"float32\")\n",
        "  for (i, input_text) in enumerate(input_texts):\n",
        "        for (t, char) in enumerate(input_text):\n",
        "          input_array[i, t] = input_token_index[\" \"]\n",
        "          if char in input_token_index:\n",
        "            input_array[i, t] = input_token_index[char]\n",
        "        input_array[i, t + 1 :] = input_token_index[\" \"]\n",
        "\n",
        "  return input_array\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N82OZs3TNcLK"
      },
      "source": [
        "\n",
        "**MACHINE TRANSLITERATOR**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYsd6Xetg7dv"
      },
      "source": [
        "class Machine_Transliterator():\n",
        "\n",
        "  ############################################# constructor for class Machine_Transliterator ##########################################\n",
        "\n",
        "  def __init__(self,max_encoder_seq_length,max_decoder_seq_length,encoder_embed_size, decoder_embed_size,\n",
        "               num_hidden_layers_in_encoder,num_hidden_layers_in_decoder,epochs, hidden_layer_size,\n",
        "               num_encoder_tokens, cell_type, num_decoder_tokens,input_token_index, target_token_index, \n",
        "               activation=\"softmax\",optimizer=\"rmsprop\"):\n",
        "    \n",
        "    self.cell_type= cell_type\n",
        "    self.hidden_layer_size = hidden_layer_size  \n",
        "    self.optimizer = optimizer\n",
        "    self.activation = activation   \n",
        "\n",
        "    #-------------------------------------- Number of hidden layers -------------------------------------\n",
        "\n",
        "    self.num_hidden_layers_in_encoder = num_hidden_layers_in_encoder\n",
        "    self.num_hidden_layers_in_decoder=num_hidden_layers_in_decoder\n",
        "\n",
        "    #-------------------------------------- sequence length -------------------------------------\n",
        "    self.max_decoder_seq_length=max_decoder_seq_length\n",
        "    self.max_encoder_seq_length=max_encoder_seq_length\n",
        "\n",
        "    #---------------------------------------------Embedding size-------------------------------------\n",
        "    self.encoder_embed_size = encoder_embed_size\n",
        "    self.decoder_embed_size = decoder_embed_size\n",
        "    \n",
        "    #-----------------information obtained after preprocessing of data-------------------------------------\n",
        "    self.num_encoder_tokens = num_encoder_tokens\n",
        "    self.num_decoder_tokens = num_decoder_tokens\n",
        "\n",
        "    #-----------------------------dictionaries----------------------------------------------------\n",
        "    self.input_token_index = input_token_index\n",
        "    self.target_token_index = target_token_index\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "#########################################function to build model ###########################################\n",
        "\n",
        "  def build_model(self):\n",
        "\n",
        "    \n",
        "    encoder_inputs = keras.Input(shape=(None,))   \n",
        "    encoder_embedding_output = tf.keras.layers.Embedding(self.num_encoder_tokens, self.encoder_embed_size)(encoder_inputs)\n",
        "\n",
        "\n",
        "#------------------------------ if cell type = LSTM -------------------------------------------------------------\n",
        "    if self.cell_type == \"lstm\":\n",
        "     #--------------------- encoder -----------------------------------\n",
        "      encoder = keras.layers.LSTM(self.hidden_layer_size, return_state=True, return_sequences=True)\n",
        "      encoder_outputs, state_h, state_c = encoder(encoder_embedding_output)\n",
        "      for i in range(1,self.num_hidden_layers_in_encoder):\n",
        "        encoder = keras.layers.LSTM(self.hidden_layer_size, return_state=True,return_sequences=True)\n",
        "        encoder_outputs, state_h, state_c = encoder(encoder_outputs)\n",
        "      encoder_states = [state_h, state_c]\n",
        "\n",
        "      #---------------------------decoder ---------------------------------------------------\n",
        "      decoder_inputs = keras.Input(shape=(None, ))      \n",
        "      decoder_embedding_output = tf.keras.layers.Embedding(self.num_decoder_tokens, self.decoder_embed_size)(decoder_inputs)\n",
        "\n",
        "      decoder = keras.layers.LSTM(self.hidden_layer_size, return_sequences=True, return_state=True)\n",
        "      decoder_outputs, _, _= decoder(decoder_embedding_output, initial_state = encoder_states)\n",
        "      for i in range(1,self.num_hidden_layers_in_decoder):\n",
        "        decoder = keras.layers.LSTM(self.hidden_layer_size, return_state=True,return_sequences=True)\n",
        "        decoder_outputs, _ , _= decoder(decoder_outputs, initial_state = encoder_states)\n",
        "\n",
        "\n",
        "\n",
        "#------------------------------ if cell type = Simple RNN -------------------------------------------------------------\n",
        "    elif self.cell_type == \"rnn\":\n",
        "      #--------------------- encoder -----------------------------------\n",
        "      encoder = keras.layers.SimpleRNN(self.hidden_layer_size, return_state=True,return_sequences=True)\n",
        "      encoder_outputs, state = encoder(encoder_embedding_output)\n",
        "      for i in range(1,self.num_hidden_layers_in_encoder):\n",
        "        encoder = keras.layers.SimpleRNN(self.hidden_layer_size, return_state=True,return_sequences=True)\n",
        "        encoder_outputs, state = encoder(encoder_outputs)\n",
        "      encoder_states = [state]\n",
        "\n",
        "      #---------------------------decoder ---------------------------------------------------\n",
        "      decoder_inputs = keras.Input(shape=(None,))      \n",
        "      decoder_embedding_output = tf.keras.layers.Embedding(self.num_decoder_tokens, self.decoder_embed_size)(decoder_inputs)\n",
        "\n",
        "      decoder = keras.layers.SimpleRNN(self.hidden_layer_size, return_state=True,return_sequences=True)\n",
        "      decoder_outputs, _ = decoder(decoder_embedding_output, initial_state = encoder_states)\n",
        "      for i in range(1,self.num_hidden_layers_in_decoder):\n",
        "        decoder = keras.layers.SimpleRNN(self.hidden_layer_size, return_state=True,return_sequences=True)\n",
        "        decoder_outputs, _= decoder(decoder_outputs, initial_state = encoder_states)\n",
        "\n",
        "\n",
        "\n",
        "#------------------------------ if cell type = GRU -------------------------------------------------------------\n",
        "    elif self.cell_type == \"gru\":\n",
        "      #--------------------- encoder -----------------------------------\n",
        "      encoder = keras.layers.GRU(self.hidden_layer_size, return_state=True,return_sequences=True)\n",
        "      encoder_outputs, state = encoder(encoder_embedding_output)\n",
        "      for i in range(1,self.num_hidden_layers_in_encoder):\n",
        "        encoder = keras.layers.GRU(self.hidden_layer_size, return_state=True,return_sequences=True)\n",
        "        encoder_outputs, state = encoder(encoder_outputs)\n",
        "      encoder_states = [state]\n",
        "\n",
        "      #---------------------------decoder ---------------------------------------------------\n",
        "      decoder_inputs = keras.Input(shape=(None, ))      \n",
        "      decoder_embedding_output = tf.keras.layers.Embedding(self.num_decoder_tokens, self.decoder_embed_size)(decoder_inputs)\n",
        "      \n",
        "      decoder = keras.layers.GRU(self.hidden_layer_size, return_state=True,return_sequences=True)\n",
        "      decoder_outputs, _ = decoder(decoder_inputs, initial_state = encoder_states)\n",
        "      for i in range(1,self.num_hidden_layers_in_decoder):\n",
        "        decoder = keras.layers.GRU(self.hidden_layer_size, return_state=True,return_sequences=True)\n",
        "        decoder_outputs, _ = decoder(decoder_outputs, initial_state = encoder_states)\n",
        "\n",
        "\n",
        "    decoder_dense = keras.layers.Dense(self.num_decoder_tokens, activation = self.activation)\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "#-------------------------- return final model ---------------------------------------------------------\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#########################################function for training the model ###########################################\n",
        "\n",
        "  def train_model(self,encoder_input_data,decoder_input_data,decoder_target_data,epochs,batch_size,\n",
        "                  val_encoder_input_data, val_decoder_input_data, val_decoder_target_data):\n",
        "    \n",
        "     model=self.build_model()\n",
        "\n",
        "     #-----------------compile the model -------------------------------------\n",
        "     model.compile(\n",
        "         optimizer=self.optimizer,\n",
        "         loss=\"categorical_crossentropy\",\n",
        "         metrics=[\"accuracy\"]\n",
        "         )      \n",
        "     model.fit(\n",
        "        [encoder_input_data, decoder_input_data],\n",
        "        decoder_target_data,\n",
        "        batch_size = batch_size,\n",
        "        epochs = epochs,\n",
        "        validation_data = ([val_encoder_input_data, val_decoder_input_data],val_decoder_target_data),)\n",
        "     return model\n",
        "\n",
        "#===================================== end of class Machine_Transliterator ==========================================\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NR4rOYbMVIUZ"
      },
      "source": [
        "**PARAMETERS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llXj7Z6LSnMz"
      },
      "source": [
        "cell_type = \"rnn\" # Type of the recurring unit\n",
        "batch_size = 64  # Batch size for training.\n",
        "epochs = 20  # Number of epochs to train for.\n",
        "hidden_layer_size= 256  # Latent dimensionality of the encoding space.\n",
        "num_samples = 10000  # Number of samples to train on.\n",
        "activation = \"softmax\" #activation\n",
        "optimizer = \"Adam\"  #optimizer\n",
        "encoder_embed_size = 27 #Encoder embedsize\n",
        "decoder_embed_size = 64 #Decoder embedsize\n",
        "num_hidden_layers_in_encoder=2  # number of hidden layers in encoder\n",
        "num_hidden_layers_in_decoder=2   # number of hidden layers in decoder"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JH8MZK3IPERe"
      },
      "source": [
        "**PREPROCESSING THE DATA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYj3x6F4SNTm",
        "outputId": "17e06397-5c93-4ee7-dd47-8a055985a762"
      },
      "source": [
        "############################# preprocessing the train data ################################\n",
        "\n",
        "#---------------------------- vectorizing train data ----------------------------------\n",
        "input_details, target_details = vectorize_data(train_data_path)\n",
        "\n",
        "#------------------------converting data into one-hot representation ---------------------------\n",
        "encoder_input_data, decoder_input_data, decoder_target_data = onehot(input_details, target_details)\n",
        "\n",
        "#------------------------------- unzipping the data ----------------------------------------\n",
        "input_characters = input_details[0]\n",
        "input_texts = input_details[1]\n",
        "input_token_index = input_details[2]\n",
        "num_encoder_tokens = input_details[3]\n",
        "max_encoder_seq_length = input_details[4]\n",
        "target_characters = target_details[0]\n",
        "target_texts = target_details[1]\n",
        "target_token_index = target_details[2]\n",
        "num_decoder_tokens = target_details[3]\n",
        "max_decoder_seq_length = target_details[4]\n",
        "\n",
        "#-------------------- converting input data into appropriate embedding ----------------------\n",
        "encoder_input_data = get_input_for_embedding(input_details, encoder_embed_size)\n",
        "decoder_input_data = get_input_for_embedding(target_details, decoder_embed_size)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples: 10000\n",
            "Number of unique input tokens: 27\n",
            "Number of unique output tokens: 64\n",
            "Max sequence length for inputs: 18\n",
            "Max sequence length for outputs: 20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2PGYsKfb84-",
        "outputId": "7e110cd7-bdc2-4b1e-f09b-e58376fcc5d6"
      },
      "source": [
        "############################# preprocessing the validation data ################################\n",
        "\n",
        "#---------------------------- vectorizing validation data ----------------------------------\n",
        "val_input_details, val_target_details = vectorize_data(validation_data_path)\n",
        "\n",
        "#------------------------converting data into one-hot representation ---------------------------\n",
        "val_encoder_input_data, val_decoder_input_data, val_decoder_target_data = onehot(val_input_details, val_target_details)\n",
        "\n",
        "#------------------------------- unzipping the data ----------------------------------------\n",
        "val_input_characters = val_input_details[0]\n",
        "val_input_texts = val_input_details[1]\n",
        "val_input_token_index = val_input_details[2]\n",
        "val_num_encoder_tokens = val_input_details[3]\n",
        "val_max_encoder_seq_length = val_input_details[4]\n",
        "val_target_characters = val_target_details[0]\n",
        "val_target_texts = val_target_details[1]\n",
        "val_target_token_index = val_target_details[2]\n",
        "val_num_decoder_tokens = val_target_details[3]\n",
        "val_max_decoder_seq_length = val_target_details[4]\n",
        "\n",
        "#-------------------- converting input data into appropriate embedding ----------------------\n",
        "val_encoder_input_data = get_input_for_embedding(val_input_details, encoder_embed_size, input_details)\n",
        "val_decoder_input_data = get_input_for_embedding(val_target_details, decoder_embed_size, target_details)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples: 4358\n",
            "Number of unique input tokens: 27\n",
            "Number of unique output tokens: 64\n",
            "Max sequence length for inputs: 18\n",
            "Max sequence length for outputs: 16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWh3rwRTcDuT"
      },
      "source": [
        "**CREATING MACHINE TRANSLITERATOR**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeEXyTYcg7bF"
      },
      "source": [
        "########################### creating machine transliterator object ###############################\n",
        "machine = Machine_Transliterator(\n",
        "    max_encoder_seq_length,max_decoder_seq_length,encoder_embed_size, \n",
        "    decoder_embed_size,num_hidden_layers_in_encoder,num_hidden_layers_in_decoder,\n",
        "    batch_size, hidden_layer_size, num_encoder_tokens, cell_type, num_decoder_tokens, \n",
        "     input_token_index,target_token_index, activation, optimizer\n",
        "    )"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJlX41fVRs1x"
      },
      "source": [
        "**TRAINING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UQyh8rmdkM8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cb28fd1-3caa-4ddd-8ae4-6dcc182cb722"
      },
      "source": [
        "model = machine.train_model(\n",
        "    encoder_input_data, decoder_input_data,decoder_target_data,epochs,batch_size,\n",
        "    val_encoder_input_data, val_decoder_input_data, val_decoder_target_data\n",
        "    )"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "157/157 [==============================] - 16s 83ms/step - loss: 1.5719 - accuracy: 0.6370 - val_loss: 1.5432 - val_accuracy: 0.6599\n",
            "Epoch 2/20\n",
            "157/157 [==============================] - 12s 79ms/step - loss: 0.9155 - accuracy: 0.7627 - val_loss: 1.3959 - val_accuracy: 0.6864\n",
            "Epoch 3/20\n",
            "157/157 [==============================] - 12s 79ms/step - loss: 0.7139 - accuracy: 0.8039 - val_loss: 1.2755 - val_accuracy: 0.7044\n",
            "Epoch 4/20\n",
            "157/157 [==============================] - 12s 78ms/step - loss: 0.5998 - accuracy: 0.8314 - val_loss: 1.2007 - val_accuracy: 0.7210\n",
            "Epoch 5/20\n",
            "157/157 [==============================] - 12s 77ms/step - loss: 0.4966 - accuracy: 0.8571 - val_loss: 1.1479 - val_accuracy: 0.7352\n",
            "Epoch 6/20\n",
            "157/157 [==============================] - 12s 79ms/step - loss: 0.4286 - accuracy: 0.8735 - val_loss: 1.1225 - val_accuracy: 0.7428\n",
            "Epoch 7/20\n",
            "157/157 [==============================] - 12s 79ms/step - loss: 0.3753 - accuracy: 0.8893 - val_loss: 1.1105 - val_accuracy: 0.7518\n",
            "Epoch 8/20\n",
            "157/157 [==============================] - 12s 78ms/step - loss: 0.3361 - accuracy: 0.8991 - val_loss: 1.1484 - val_accuracy: 0.7430\n",
            "Epoch 9/20\n",
            "157/157 [==============================] - 12s 79ms/step - loss: 0.3082 - accuracy: 0.9065 - val_loss: 1.1295 - val_accuracy: 0.7545\n",
            "Epoch 10/20\n",
            "157/157 [==============================] - 13s 80ms/step - loss: 0.2792 - accuracy: 0.9145 - val_loss: 1.1522 - val_accuracy: 0.7557\n",
            "Epoch 11/20\n",
            "157/157 [==============================] - 12s 78ms/step - loss: 0.2557 - accuracy: 0.9220 - val_loss: 1.1255 - val_accuracy: 0.7593\n",
            "Epoch 12/20\n",
            "157/157 [==============================] - 12s 78ms/step - loss: 0.2289 - accuracy: 0.9301 - val_loss: 1.1174 - val_accuracy: 0.7631\n",
            "Epoch 13/20\n",
            "157/157 [==============================] - 12s 78ms/step - loss: 0.2180 - accuracy: 0.9323 - val_loss: 1.1404 - val_accuracy: 0.7586\n",
            "Epoch 14/20\n",
            "157/157 [==============================] - 13s 82ms/step - loss: 0.2027 - accuracy: 0.9368 - val_loss: 1.1185 - val_accuracy: 0.7643\n",
            "Epoch 15/20\n",
            "157/157 [==============================] - 12s 78ms/step - loss: 0.1857 - accuracy: 0.9418 - val_loss: 1.2645 - val_accuracy: 0.7536\n",
            "Epoch 16/20\n",
            "157/157 [==============================] - 12s 78ms/step - loss: 0.1841 - accuracy: 0.9426 - val_loss: 1.1936 - val_accuracy: 0.7621\n",
            "Epoch 17/20\n",
            "157/157 [==============================] - 12s 79ms/step - loss: 0.1614 - accuracy: 0.9497 - val_loss: 1.2008 - val_accuracy: 0.7616\n",
            "Epoch 18/20\n",
            "157/157 [==============================] - 12s 78ms/step - loss: 0.1594 - accuracy: 0.9499 - val_loss: 1.2546 - val_accuracy: 0.7560\n",
            "Epoch 19/20\n",
            "157/157 [==============================] - 12s 78ms/step - loss: 0.1466 - accuracy: 0.9527 - val_loss: 1.2409 - val_accuracy: 0.7603\n",
            "Epoch 20/20\n",
            "157/157 [==============================] - 12s 78ms/step - loss: 0.1409 - accuracy: 0.9551 - val_loss: 1.2893 - val_accuracy: 0.7558\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}