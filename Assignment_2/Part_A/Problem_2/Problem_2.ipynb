{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Problem_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWejop-kpJdc"
      },
      "source": [
        "**IMPORTING REQUIRED LIBRARIES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTCAHfTc66zf"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import cv2\n",
        "import pathlib\n",
        "import glob\n",
        "import shutil\n",
        "import os\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwlB2BffHI5x"
      },
      "source": [
        "**CONNECTING TO WANDB**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEvg7mOa9vca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93dbdaac-c89c-49a1-ef6f-ebeb42a4b198"
      },
      "source": [
        "#---------------------------------------install and import wandb -------------------------------------------------\n",
        "!pip install wandb -qqq\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.1MB 5.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 163kB 21.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 19.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 9.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 8.8MB/s \n",
            "\u001b[?25h  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEA578roZ93I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5492829-1197-410f-8723-c3e4c8cb843b"
      },
      "source": [
        "#--------------------------------------------login to wandb --------------------------------------------\n",
        "!wandb login"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TET9FvESoMP5"
      },
      "source": [
        "\n",
        "**IMPORTING THE DATA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2VpRP5MxlKf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be130e6d-f8a1-4498-9273-b15a496df976"
      },
      "source": [
        "######################################### caution : terminal commands #######################################################\n",
        "\n",
        "#-------------------------------------empty the datasets forlder before downloading the dataset -------------------------------------\n",
        "\n",
        "%cd\n",
        "%cd .keras/datasets/\n",
        "!rm -r *"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root\n",
            "[Errno 2] No such file or directory: '.keras/datasets/'\n",
            "/root\n",
            "rm: cannot remove '*': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cHTe-1z7tdF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44edf1fc-6c27-4aae-ec33-c05c1ff7ba9a"
      },
      "source": [
        "########################################### download data from given url ###############################################3\n",
        "\n",
        "dataset_url = \"https://storage.googleapis.com/wandb_datasets/nature_12K.zip\"\n",
        "data_dir = tf.keras.utils.get_file('nature_12K', origin=dataset_url, extract=True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/wandb_datasets/nature_12K.zip\n",
            "3816693760/3816687935 [==============================] - 63s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3n--Egiv5E4C"
      },
      "source": [
        "**SPLITTING THE DATA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdH9jqUI3VXu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79e8a13c-dd9c-4acf-8f2b-bc9543474a23"
      },
      "source": [
        "#---------------------------------------------caution : terminal commands ----------------------------------------------\n",
        "\n",
        "%cd\n",
        "%cd .keras/datasets/inaturalist_12K\n",
        "%mv val test\n",
        "!mkdir valid"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root\n",
            "/root/.keras/datasets/inaturalist_12K\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOL-kw6ZFyXz"
      },
      "source": [
        "#################################### split train data into validation set and training set ###################################\n",
        "\n",
        "data_folder = '/root/.keras/datasets/inaturalist_12K'\n",
        "os.chdir(data_folder)\n",
        "\n",
        "folder_names = ['Amphibia', 'Animalia', 'Arachnida', 'Aves', 'Fungi', 'Insecta', 'Mammalia', 'Mollusca', 'Plantae', 'Reptilia' ]\n",
        "for i in range(0,10):\n",
        "  source = data_folder + \"/train/\" +folder_names[i]  \n",
        "  orig_files = os.listdir(source)\n",
        "  chosen_indexes = random.sample(range(0, len(orig_files)-1), 100)\n",
        "\n",
        "  destination = data_folder + \"/valid/\"\n",
        "  os.chdir(destination)\n",
        "  os.system('mkdir'+' '+str(folder_names[i]))\n",
        "  destination = destination + folder_names[i]\n",
        "  for j in range(0,100):\n",
        "    shutil.move(   source +\"/\" + str(   orig_files[   chosen_indexes[j]  ]   )  , destination)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPbh7g46gDn3"
      },
      "source": [
        "####################################### Correcting the directory location ####################################################\n",
        "\n",
        "#data_dir = '/root/.keras/datasets/nature_12K'\n",
        "\n",
        "data_dir = data_dir.split('/')\n",
        "data_dir.remove('nature_12K')\n",
        "data_dir.append('inaturalist_12K')\n",
        "data_dir = '/'.join(data_dir)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmVVktRSklrV"
      },
      "source": [
        "#-------------------------------------Taking the train data--------------------------------------------------------\n",
        "train_data_dir_path = data_dir + '/train'\n",
        "train_data_dir = pathlib.Path(train_data_dir_path)\n",
        "\n",
        "#--------------------------------------Taking the validation data-----------------------------------------------\n",
        "valid_data_dir_path = data_dir + '/valid'\n",
        "valid_data_dir = pathlib.Path(valid_data_dir_path)\n",
        "\n",
        "#----------------------------------------Taking the test data-----------------------------------------------\n",
        "test_data_dir_path = data_dir + '/test'\n",
        "test_data_dir = pathlib.Path(test_data_dir_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajBb-R_hlR7A"
      },
      "source": [
        "#------------------------------------------Parameters for the images-------------------------------------------------\n",
        "batch_size = 32\n",
        "img_height = 180\n",
        "img_width = 180"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyDCKLnkzhf7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c24d7b11-666f-4f58-9fc9-9ff1190d129d"
      },
      "source": [
        "####################################### converting the image data into dataset ############################################\n",
        "\n",
        "#-------------------------------------training dataset -----------------------------------------------------\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  train_data_dir,\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size\n",
        "  )\n",
        "\n",
        "#----------------------------------------------validation dataset -----------------------------------------------\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  valid_data_dir,\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8999 files belonging to 10 classes.\n",
            "Found 1000 files belonging to 10 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuRYfT4THW2h"
      },
      "source": [
        "**CONVOLUTIONAL NEURAL NETWORK**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJO2h0D-7T0u"
      },
      "source": [
        "class CNN():\n",
        "\n",
        "############################################# constructor for class CNN ##########################################\n",
        "  def __init__(self, filter_count, filter_size, active_func, dense_neurons_count, maxpool_size, output_neurons_count, data_augmentation, drop_out, batch_normalisation, optimizer='adam'):\n",
        "    self.filter_count = filter_count\n",
        "    self.filter_size = filter_size\n",
        "    self.active_func = active_func\n",
        "    self.dense_neurons_count = dense_neurons_count\n",
        "    self.maxpool_size = maxpool_size\n",
        "    self.num_classes = output_neurons_count\n",
        "    self.optimizer = optimizer\n",
        "    self.data_augmentation = data_augmentation\n",
        "    self.drop_out = drop_out\n",
        "    self.batch_normalisation = batch_normalisation\n",
        "\n",
        "    #harcoded values\n",
        "    self.img_height = 180\n",
        "    self.img_width = 180\n",
        "\n",
        "    #creating the CNN model\n",
        "    self.model = self.create_model(5)\n",
        "\n",
        "\n",
        "############################################### function to create model ####################################################\n",
        "  def create_model(self, num_layers):    \n",
        "    \n",
        "    # #-------------------------------------------creating the CNN model -----------------------------------------------------\n",
        "\n",
        "    augmentations = [     layers.experimental.preprocessing.RandomFlip(\"horizontal\", input_shape=(img_height, img_width, 3)),\n",
        "                          layers.experimental.preprocessing.RandomRotation(0.1),\n",
        "                          layers.experimental.preprocessing.RandomZoom(0.1)\n",
        "                    ]\n",
        "\n",
        "    data_augment = keras.Sequential(augmentations)\n",
        "\n",
        "    layerslist = []\n",
        "\n",
        "    #------------------Data Augmentation---------------------------------------\n",
        "    if self.data_augmentation == \"Yes\":\n",
        "      layerslist.append(data_augment)\n",
        "\n",
        "    #------------------Including a normalisation layer in the model---------------------------------------\n",
        "    layerslist.append(layers.experimental.preprocessing.Rescaling(1./255, input_shape=(self.img_height, self.img_width, 3)))\n",
        "\n",
        "    #------------------Creating (convolution,activation,maxpool) layers in the model---------------------------------------\n",
        "    for i in range(0,num_layers):\n",
        "      layerslist.append(layers.Conv2D(self.filter_count[i], self.filter_size[i], padding='same', activation = self.active_func[i]))\n",
        "      if self.batch_normalisation[i] == 'Yes':\n",
        "        layerslist.append(layers.BatchNormalization( axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True,\n",
        "                                                     beta_initializer='zeros', gamma_initializer='ones',\n",
        "                                                     moving_mean_initializer='zeros', moving_variance_initializer='ones'))\n",
        "      layerslist.append(layers.MaxPooling2D(pool_size = self.maxpool_size[i]))\n",
        "\n",
        "    layerslist.append(layers.Dropout(self.drop_out))\n",
        "    layerslist.append(layers.Flatten())\n",
        "    layerslist.append(layers.Dense(self.dense_neurons_count,  activation = self.active_func[num_layers]))\n",
        "    layerslist.append(layers.Dense(self.num_classes))\n",
        "\n",
        "    model = Sequential(layerslist)\n",
        "\n",
        "    # ---------------------------------------compiling the CNN model ----------------------------------------------------------\n",
        "    model.compile(\n",
        "    optimizer = self.optimizer,\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'] )\n",
        "\n",
        "\n",
        "  #--------------------------------------------- return final model --------------------------------------------------------\n",
        "    return model \n",
        "\n",
        "\n",
        "  \n",
        "############################################## function for training the model ##################################################3\n",
        "  def train(self, train_ds, val_ds, epochs = 10):\n",
        "\n",
        "    #------------------------------------caching the datasets -----------------------------------------------\n",
        "    AUTOTUNE = tf.data.AUTOTUNE\n",
        "    train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
        "    val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "    #------------------------------------ training the model -----------------------------------------------\n",
        "    history = self.model.fit( train_ds, validation_data=val_ds, epochs=epochs, callbacks=[WandbCallback()] )\n",
        "    \n",
        "    return history\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "############################################ function for summary of model ###########################################3\n",
        "  def model_summary(self):\n",
        "    return self.model.summary()\n",
        "\n",
        "#=============================================== end of class CNN ======================================================\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snrZx8tFDz7R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a0acbbe-283a-4350-af56-6d73a0937947"
      },
      "source": [
        "sweep_config={\n",
        "              \"method\":\"random\",\n",
        "              'metric' : {\n",
        "                            'name' : 'val_accuracy',\n",
        "                            'goal' : 'maximize',\n",
        "                         },\n",
        "          \"parameters\" : {\n",
        "                            \"filter_count\":{\"values\":[[16,32,64,64,64],[16,32,64,128,256], [256, 128, 64, 32, 16]]},\n",
        "                            \"filter_size\":{\"values\":[[3,3,3,3,3]]},\n",
        "                            \"data_augmentation\":{\"values\":[\"Yes\",\"No\"]},\n",
        "                            \"drop_out\":{\"values\":[0.15,0.2,0.25]},\n",
        "                            \"num_epochs\":{\"values\":[5,10]},\n",
        "                            \"batch_normalisation\":{\"values\":[[\"Yes\",\"Yes\",\"Yes\",\"Yes\",\"Yes\"],[\"No\",\"No\",\"No\",\"No\",\"No\"]]}\n",
        "                         }\n",
        "              }\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"pretrained\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Create sweep with ID: aklg700n\n",
            "Sweep URL: https://wandb.ai/pranayrajparisha/pretrained/sweeps/aklg700n\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwdBnPOjEAXI"
      },
      "source": [
        "def run():\n",
        "\n",
        "  wb = wandb.init()\n",
        "  config = wb.config\n",
        "\n",
        "  # sweep parameters\n",
        "  filter_count = config.filter_count\n",
        "  filter_size = config.filter_size\n",
        "  data_augmentation = config.data_augmentation\n",
        "  drop_out = config.drop_out\n",
        "  batch_normalisation = config.batch_normalisation\n",
        "  epochs = config.num_epochs\n",
        "\n",
        "  #fixed parameters\n",
        "  maxpool_size = [2,2,2,2,2]\n",
        "  active_func = ['relu']*6\n",
        "  dense_neurons_count = 120\n",
        "  output_neurons_count = 10\n",
        "  \n",
        "  \n",
        "  cnn = CNN(filter_count, filter_size, active_func, dense_neurons_count, maxpool_size, output_neurons_count, data_augmentation, drop_out, batch_normalisation)\n",
        "  history = cnn.train(train_ds, val_ds, epochs)\n",
        "  \n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVg8H7G7LGYU"
      },
      "source": [
        "wandb.agent(sweep_id, run)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YV1VWrEfUBn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMntTNXf4umO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Alt_qTNgVLUu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "In0tqPqSVLI6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}